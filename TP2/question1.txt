// Où X = [X_1, X_2, ... X_n, 1]
// W est une matrice de poids avec une colonne ajoutée à la fin pour représenter les biais 
// W_l représente donc la matrice de poids associée à chaque L layer 
// Il y a L layers et la couche de sortie est L+1
// alpha est le taux d'apprentissage
// in_l est la préactiviation du layer l 
// A_l est l'activation du layer l 
// le réseau contient L couches cachées


		A_l-1 <- X // l'entrée est chargée dans le vecteur A d'entrée

		POUR l=1 à L+1 FAIRE
			
			in_l <- (W_l)(A_l-1)
			A_l <- 1 / (1 + exp(-in_l))


		// Soit Y la sortie désirée pour et A la sortie obtenue 

		delta <- ((1 / (1 + exp(-in_L+1)))*( 1 - 1 / (1 + exp(-in_L+1))))  * (Y-A_L+1)

		POUR l=L à 1 FAIRE

			delta_l <-((1 / (1 + exp(-in_l)))*( 1 - 1 / (1 + exp(-in_l)))) * (W_l)(delta)
			W_l <- W_l+ (alpha)(A_l)(delta_l+1)

